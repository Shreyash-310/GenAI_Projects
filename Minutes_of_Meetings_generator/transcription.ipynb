{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transcribe_audio_huggingface1(audio_file):\n",
    "#     # Load the pre-trained Speech2Text model and processor\n",
    "#     processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "#     model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "    \n",
    "#     # Load and preprocess the audio file\n",
    "#     speech_array, sampling_rate = torchaudio.load(audio_file)\n",
    "#     speech = speech_array.squeeze().numpy()\n",
    "\n",
    "#     # Ensure the sample rate matches the model's requirements\n",
    "#     if sampling_rate != 16000:\n",
    "#         resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "#         speech_array = resampler(speech_array)\n",
    "\n",
    "#     # Process the audio data\n",
    "#     inputs = processor(speech_array.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "#     # Generate transcription (speech-to-text)\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(inputs[\"input_features\"])\n",
    "#         transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "#     print(\"Transcription:\", transcription)\n",
    "#     return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with your audio file path (wav or mp3 file)\n",
    "# transcribe_audio_huggingface('audio_files\\S2T_test_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_huggingface(audio_file):\n",
    "    # Load the pre-trained Speech2Text model and processor\n",
    "    processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "    model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "    \n",
    "    # Set device (GPU if available, otherwise CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and preprocess the audio file using soundfile (for WAV files)\n",
    "    speech_array, sampling_rate = sf.read(audio_file)\n",
    "\n",
    "    # Ensure the sample rate matches the model's requirements\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(torch.tensor(speech_array, dtype=torch.float32).unsqueeze(0)).squeeze().numpy()\n",
    "\n",
    "    speech_array = speech_array.astype('float32')\n",
    "\n",
    "    # Process the audio data\n",
    "    inputs = processor(speech_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    input_features = inputs[\"input_features\"].to(device).float()\n",
    "\n",
    "    # Generate transcription (speech-to-text)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"Transcription:\", transcription)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: ah oh mamma came on ah they shall go back i shall go back i'll put a pullet down i'll put it back back back back back back back back and back back back back back back and back back back back back and back back back back back and back back back back and back back back back and back back back and back and back back back and back back back back and back and back and back and back back back and back back and back and back and back back back back back back back and back back back back back back back back back back back back back back back and back back back back back back back back back back back back and back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ah oh mamma came on ah they shall go back i shall go back i'll put a pullet down i'll put it back back back back back back back back and back back back back back back and back back back back back and back back back back back and back back back back and back back back back and back back back and back and back back back and back back back back and back and back and back and back back back and back back and back and back and back back back back back back back and back back back back back back back back back back back back back back back and back back back back back back back back back back back back and back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back back\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function with your audio file path (WAV file)\n",
    "transcribe_audio_huggingface(\"audio_files/S2T_test_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
